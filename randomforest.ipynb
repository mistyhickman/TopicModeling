{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import shap\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources if not already available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocessing Utilities\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: tokenization, lemmatization, and stopword removal.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words\n",
    "    ]\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(text):\n",
    "    \"\"\"Extract additional features from text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {\n",
    "            'length': 0,\n",
    "            'word_count': 0,\n",
    "            'sentiment_score': 0\n",
    "        }\n",
    "        \n",
    "    # Basic metrics\n",
    "    length = len(text)\n",
    "    word_count = len(text.split())\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiment_score = sid.polarity_scores(text)['compound']\n",
    "    \n",
    "    return {\n",
    "        'length': length,\n",
    "        'word_count': word_count,\n",
    "        'sentiment_score': sentiment_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    \"\"\"Add time-based features to the dataframe.\"\"\"\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "    df['end_date'] = pd.to_datetime(df['end_date'])\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df['day_of_week'] = df['start_date'].dt.dayofweek\n",
    "    df['hour_of_day'] = df['start_date'].dt.hour\n",
    "    df['month'] = df['start_date'].dt.month\n",
    "    df['is_weekend'] = df['start_date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Directories\n",
    "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "run_dir = f\"C:/Users/misty.hickman/OneDrive/Documents/dev_work/TopicModeling/model_artifacts/run_{today}\"\n",
    "visualizations_dir = f\"{run_dir}/visualizations\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "os.makedirs(visualizations_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = \"research_review_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Save a sample of original data before preprocessing\n",
    "sample_data = {\n",
    "    'review': df['reviews'].iloc[0],\n",
    "    'response': df['response_reviews'].iloc[0],\n",
    "    'start_date': df['start_date'].iloc[0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "df['processed_reviews'] = df['reviews'].apply(preprocess_text)\n",
    "df['processed_response_reviews'] = df['response_reviews'].apply(preprocess_text)\n",
    "df['combined_text'] = df['processed_reviews'] + \" \" + df['processed_response_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features with explicit column names\n",
    "review_features = pd.DataFrame([extract_text_features(text) for text in df['reviews']], \n",
    "                             columns=['length', 'word_count', 'sentiment_score'])\n",
    "response_features = pd.DataFrame([extract_text_features(text) for text in df['response_reviews']], \n",
    "                               columns=['length', 'word_count', 'sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prefixes to column names\n",
    "review_features.columns = ['review_' + col for col in review_features.columns]\n",
    "response_features.columns = ['response_' + col for col in response_features.columns]\n",
    "\n",
    "# Add text features to main dataframe\n",
    "df = pd.concat([df, review_features, response_features], axis=1)\n",
    "\n",
    "# Add time features\n",
    "df = add_time_features(df)\n",
    "\n",
    "# Calculate duration\n",
    "df['duration_time'] = (df['end_date'] - df['start_date']).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LDA\n",
    "texts = df['combined_text'].str.split()\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train LDA Model\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=num_topics,\n",
    "    id2word=dictionary,\n",
    "    passes=10,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic distributions as features\n",
    "def get_topic_distribution(bow):\n",
    "    return [prob for _, prob in lda_model.get_document_topics(bow, minimum_probability=0)]\n",
    "\n",
    "topic_distributions = [get_topic_distribution(bow) for bow in corpus]\n",
    "topic_features = pd.DataFrame(topic_distributions, columns=[f'topic_{i}' for i in range(num_topics)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "numerical_features = df[[\n",
    "    'review_length', 'review_word_count', 'review_sentiment_score',\n",
    "    'response_length', 'response_word_count', 'response_sentiment_score',\n",
    "    'day_of_week', 'hour_of_day', 'month', 'is_weekend'\n",
    "]].values\n",
    "\n",
    "X = np.hstack([X_tfidf.toarray(), numerical_features, topic_distributions])\n",
    "y = df['duration_time'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "print(f\"R-squared Score: {r2:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations directory if it doesn't exist\n",
    "os.makedirs(visualizations_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Duration')\n",
    "plt.ylabel('Predicted Duration')\n",
    "plt.title('Actual vs Predicted Duration')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, 'actual_vs_predicted.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Duration')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, 'residuals.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': list(tfidf_vectorizer.get_feature_names_out()) + \n",
    "               ['review_length', 'review_word_count', 'review_sentiment_score',\n",
    "                'response_length', 'response_word_count', 'response_sentiment_score',\n",
    "                'day_of_week', 'hour_of_day', 'month', 'is_weekend'] +\n",
    "               [f'topic_{i}' for i in range(num_topics)],\n",
    "    'importance': best_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, 'feature_importance.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot SHAP values for feature importance interpretation\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test_scaled[:100])  # Using first 100 samples for visualization\n",
    "\n",
    "# Create correct feature names list matching the actual features\n",
    "feature_names = (\n",
    "    list(tfidf_vectorizer.get_feature_names_out()) + \n",
    "    ['review_length', 'review_word_count', 'review_sentiment_score',\n",
    "     'response_length', 'response_word_count', 'response_sentiment_score',\n",
    "     'day_of_week', 'hour_of_day', 'month', 'is_weekend'] +\n",
    "    [f'topic_{i}' for i in range(num_topics)]\n",
    ")\n",
    "\n",
    "# Ensure we have the correct number of feature names\n",
    "if len(feature_names) != X_test_scaled.shape[1]:\n",
    "    print(f\"Warning: Feature names count ({len(feature_names)}) doesn't match feature count ({X_test_scaled.shape[1]})\")\n",
    "    # Use generic feature names if there's a mismatch\n",
    "    feature_names = [f\"Feature {i}\" for i in range(X_test_scaled.shape[1])]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_test_scaled[:100],\n",
    "    feature_names=feature_names,\n",
    "    show=False,\n",
    "    plot_size=(12, 8)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, 'shap_summary.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model and artifacts\n",
    "model_path = os.path.join(run_dir, \"random_forest_model.pkl\")\n",
    "pd.to_pickle(best_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics with MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"mape\", mape)\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction function for new data\n",
    "def predict_duration(review_text, response_text, start_date):\n",
    "    \"\"\"\n",
    "    Predict duration for new review and response\n",
    "    \n",
    "    Parameters:\n",
    "    review_text (str): The review text\n",
    "    response_text (str): The response text\n",
    "    start_date (str): The start date in format 'YYYY-MM-DD HH:MM:SS'\n",
    "    \n",
    "    Returns:\n",
    "    float: Predicted duration in hours\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_review = preprocess_text(review_text)\n",
    "    processed_response = preprocess_text(response_text)\n",
    "    combined_text = processed_review + \" \" + processed_response\n",
    "    \n",
    "    # Extract text features\n",
    "    review_feats = extract_text_features(review_text)\n",
    "    response_feats = extract_text_features(response_text)\n",
    "    \n",
    "    # Create temporary dataframe for time features\n",
    "    temp_df = pd.DataFrame({\n",
    "        'start_date': [pd.to_datetime(start_date)]\n",
    "    })\n",
    "    temp_df = add_time_features(temp_df)\n",
    "    \n",
    "    # Transform text using saved vectorizer\n",
    "    text_features = tfidf_vectorizer.transform([combined_text]).toarray()\n",
    "    \n",
    "    # Get topic distribution\n",
    "    bow = dictionary.doc2bow(combined_text.split())\n",
    "    topic_dist = get_topic_distribution(bow)\n",
    "    \n",
    "    # Combine all features\n",
    "    numerical_feats = np.array([\n",
    "        review_feats['length'], review_feats['word_count'], review_feats['sentiment_score'],\n",
    "        response_feats['length'], response_feats['word_count'], response_feats['sentiment_score'],\n",
    "        temp_df['day_of_week'].iloc[0], temp_df['hour_of_day'].iloc[0], \n",
    "        temp_df['month'].iloc[0], temp_df['is_weekend'].iloc[0]\n",
    "    ])\n",
    "    \n",
    "    X_new = np.hstack([text_features, numerical_feats.reshape(1, -1), np.array(topic_dist).reshape(1, -1)])\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    \n",
    "    return best_model.predict(X_new_scaled)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After all the model training, use the saved sample data for prediction\n",
    "example_prediction = predict_duration(\n",
    "    sample_data['review'],\n",
    "    sample_data['response'],\n",
    "    sample_data['start_date']\n",
    ")\n",
    "print(f\"\\nExample Prediction: {example_prediction:.2f} hours\")\n",
    "\n",
    "# Compare with actual duration if you want to validate\n",
    "actual_duration = (pd.to_datetime(df['end_date'].iloc[0]) - pd.to_datetime(df['start_date'].iloc[0])).total_seconds() / 3600\n",
    "print(f\"Actual Duration: {actual_duration:.2f} hours\")\n",
    "print(f\"Prediction Error: {abs(actual_duration - example_prediction):.2f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
